---
title: "Lab 08 | R continued"
author: "Sean Mussenden"
date: "10/26/2019"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE, paged.print=TRUE)
```

## Objective

The purpose of this lab is to continue learning a journalistic approach to data analysis in R. 

We will continue to do things learned in previous labs:

* Writing R code for data analysis and exploration in the R Studio environment, using R projects (.Rproj) and R markdown files (.Rmd).  
* Loading, cleaning, making sense of and analyzing data using the Tidyverse framework of packages by selecting certain columns, sorting and filtering
* Create new columns in our data set based on information in other columns.   
* Summarizing data by grouping and calculating min, max, median and mean values.    
* Store changes on GitHub.
* Join together two related data sets on a common field.  
* Do some additional data cleaning, including fixing dates so we can work with them. 
* To make visualizations.

Today, we'll also learn:

* To get data a variety of ways: downloading, pulling from an API, and some light scraping. 

## How this works, tasks, turning it in, getting help

This document is mostly set up for you to follow along and run code that I have written, and listen to me explain it.  

At several points throughout this document, you will see the word **Task**.  

That indicates I'm expecting you to modify the file I've given you, usually by creating a codeblock and writing some custom code. 

When you are finished, you should save your R markdown file and Knit it as an HTML file. 

You should upload it to GitHub, using GitHub desktop. 

And the links to your project is what you'll post on ELMS. 

Need help?  You are welcome to do the following things:

* Use Google or search Stack Overflow. Try searching for your error message or translating your problem into basic terms.
* Check out the excellent [R for Data Science](https://r4ds.had.co.nz/index.html)
* Take a look at the [Cheatsheets](https://www.rstudio.com/resources/cheatsheets/) and [Tidyverse documentation](https://www.tidyverse.org/).
  * [RStudio cheatsheet](https://www.rstudio.com/resources/cheatsheets/#ide)
  * [Readr and Tidyr cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf) and [Readr documentation](https://readr.tidyverse.org/) and [Tidyr documentation](https://tidyr.tidyverse.org/reference/index.html).
  * [Dplyr cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf) and [Dplyr documentation](https://dplyr.tidyverse.org/)
  * [Lubridate cheatsheet](https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf) and [Lubridate documentation](https://lubridate.tidyverse.org/).
  * [GGPlot cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf) and [GGplot Documentation](https://ggplot2.tidyverse.org/reference/)
  * [Mapping Tutorials](https://walkerke.github.io/tidycensus/articles/spatial-data.html)
  * [GitHub desktop help](https://help.github.com/en/desktop/getting-started-with-github-desktop)
* If you're really stuck, message me on ELMS. 

## Setup

Take the following steps to set up your document:

1. Download the ZIP file and open the folder on your desktop. 
2. Create a new folder in your git repo and move it in there. Unzip the folder.
3. Open this file in RStudio.
4. Rename this file "lab_08_FIRSTNAME_LASTNAME.Rmd".
5. Create a new R project inside of this folder, which will set the working directory in this folder.   

## Load Packages

We're loading seven packages today. five of these we've loaded previously: the Tidyverse (for general data science goodness and visualizing charts and maps), janitor (for data cleaning), arcos (for loading WaPo opioid data) and tidycensus.

We're also going to load two new packages: [mapview](https://r-spatial.github.io/mapview/) (for making interactive maps) and [ggthemes](https://rdrr.io/cran/ggthemes/) (for doing cool styling stuff).  

**Task**: In the code block below, load the packages we'll need for today. 

```{r}

# Load Tidyverse, janitor and arcos, tidycensus, mapview, ggthemes, scales
library(tidyverse)
library(janitor)
library(arcos)
library(tidycensus)
library(rvest)

```

## Using the ARCOS R Package

For this exercise, we will be working with subsets of the DEA's ARCOS database, which documented shipments of 76 billion opioid pills between 2006 and 2012, during the peak of the opioid epidemic. 

The data was obtained after a lengthy legal battle by the Washington Post and the Charleston Gazette-Mail, and released by the Washington Post in raw and aggregated form. [Washington Post "Digging into the DEA's pain pill database" page](https://www.washingtonpost.com/graphics/2019/investigations/dea-pain-pill-database/).

A data dictionary is available here: [ARCOS Registrant Handbook](https://www.deadiversion.usdoj.gov/arcos/handbook/full.pdf).

We're going to load the data exclusively from the arcos R package API [ARCOS API](https://wpinvestigative.github.io/arcos/) produced by the Washington Post, instead of uploading csvs and tsvs. 

Remember, we need to store a password of sorts -- called an API key -- that will give us permission to access their data.  Here's a list of [API keys that will work](https://github.com/wpinvestigative/arcos-api/blob/master/keys/keys.txt).  

Let's store the key first. 

```{r}
# store one of our API keys as an object called key
key <- "uO4EK6I"
```


### Scraping Data from the Internet

There's lots of great information on the web that is not available for download as a CSV, but instead exists as data on an HTML page. 

Fortunately, the handy rvest package makes it easy to scrape that data into a format we can work with in R. 
If you go to the [GitHub repo](https://github.com/rtburg/NICAR2020-Intro-to-R) for this course, then into the data folder, then click on grandparentsR1.csv, we see an html table that contains the information we've been working with on grandparents caring for grandchildren. 

If we use our web browser to inspect element, we see it's an html table.  Which means it should be fairly trivial to scrape it. 

First, let's use the read_html function in rvest to pull in the raw html content from the page, and store it as an object called grandparents_scrape. 

```{r}
# http://bit.ly/r3_data
# grandparents_scrape <-read_html("https://github.com/rtburg/NICAR2020-Intro-to-R/blob/master/data/grandparentsR1.csv")

opioid_scrape <-read_html("https://www.drugabuse.gov/drugs-abuse/opioids/opioid-summaries-by-state")

```

In our environment window, we see it comes in as a "nested list".  We can examine it in our environment window, and see it's structured like an HTML page. 

Now, let's strip out all the junk we don't want, and just keep the thing we want: the table with our info. 

```{r}
opioid_scrape <-read_html("https://www.drugabuse.gov/drugs-abuse/opioids/opioid-summaries-by-state") %>%
  html_nodes('table')
```

We can examine the results in the environment window.  It kept the table head and table body, but it's kinda unreadable. Luckily, we can use the html_table() convenience function to clean it up. header=1 says to use the first row as a header. 

```{r}
opioid_scrape <- read_html("https://www.drugabuse.gov/drugs-abuse/opioids/opioid-summaries-by-state") %>%
  html_nodes('table') %>%
  html_table(header=1, fill=TRUE) 
```

We're getting closer.  But it's still a nested list, not super usable for our purposes. We can add an as.data.frame() function to make it a data frame. When you view it in the environment window, it looks like a dataframe now. 

```{r}
opioid_scrape <- read_html("https://www.drugabuse.gov/drugs-abuse/opioids/opioid-summaries-by-state") %>%
  html_nodes('table') %>%
  html_table(header=1, fill=TRUE)  %>%
  as.data.frame()
```

STILL HAVE TO CLEAN IT UP

It's got one extra blank column. We can remove that with a select() statement, saying drop the first column. 

```{r}
grandparents_scrape <- read_html("https://github.com/rtburg/NICAR2020-Intro-to-R/blob/master/data/grandparentsR1.csv") %>%
  html_nodes('table') %>%
  html_table(header=1) %>%
  as.data.frame() %>%
  select(-1)
```



### Loading Local Data

If you've taken earlier classes in this sequence, you'll have a good sense of how to load data locally using the read_csv() function from the readr package.

We're going to read in the same data set we've been working with in earlier classes, of grandparents in Louisiana counties who are living with or are responsible for their grandchildren. 

Let's read in the data now, and store it as an object called grandparents_local
grandparents_local <- read_csv("data/grandparentsR1.csv")

```{r}
# Note: the path may be different on class pcs




```{r}

arcos_county_pills_per_year <- summarized_county_annual(key = key) %>%
  clean_names()

arcos_county_population_per_year <- county_population(key = key) %>%
  clean_names()

pills_population <- arcos_county_population_per_year %>%
  left_join(arcos_county_pills_per_year, by = c("countyfips", "year", "buyer_county","buyer_state")) %>%
  group_by(countyfips, buyer_county, buyer_state) %>%
  summarise(average_pills_per_year = mean(dosage_unit),
            average_population_per_year = mean(population)) %>%
  mutate(average_pills_per_person = round(average_pills_per_year/average_population_per_year,2))

```

# Load Death Rate Data

```{r}
opioid_deaths <- read_tsv("data/2006-2012.txt") %>%
  clean_names() %>%
  filter(!str_detect(age_adjusted_rate, "Unreliable|Suppressed|Missing")) %>%
  select(county_code, county, deaths, age_adjusted_rate) %>%
  mutate(deaths = as.numeric(deaths),
         age_adjusted_rate = as.numeric(age_adjusted_rate))

```

# Join 

End up with about 1066 records.  For smallest counties, where weren't lot of pill shipments, death rate data is unreliable. 

```{r}
death_rate_pills <- pills_population %>%
  inner_join(opioid_deaths, by=c("countyfips" = "county_code"))

```

# Examine Table


# Explore relationship
Fairly tightly packed, but lots of exceptions, line moving up

```{r}
ggplot(death_rate_pills) +
  geom_point(aes(average_pills_per_person, age_adjusted_rate)) +
  geom_smooth(aes(average_pills_per_person, age_adjusted_rate), method = "lm", se = FALSE)  +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(labels = comma)  +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x="Average Annual Pills Per Person", y="age_adjusted Annual Opioid Death Rate", title="", caption = "Source: DEA ARCOS database, via Washington Post")
```

# Exact Correlation

```{r}

death_rate_pills %>%
  ungroup() %>%
  select(age_adjusted_rate, average_pills_per_person) %>%
  correlate()

```

```{r}
death_rate_pills %>%
  ungroup() %>%
  select_if(is.numeric) %>%
  correlate()

```

# Now let's pull in census data 

```{r}
census_api_key("549950d36c22ff16455fe196bbbd01d63cfbe6cf")
# acs_variables <- load_variables(2017, "acs5" )
county_median_household_income <- get_acs(geography = "county", 
              variables="B19013_001", year=2012, geometry = FALSE)

```

# Join it to our other data

```{r}
death_rate_pills_income <- death_rate_pills %>%
  inner_join(county_median_household_income, by=c("countyfips" = "GEOID")) %>%
  rename(median_household_income = estimate)

glimpse(death_rate_pills_income)
```

# Task: make a scatterplot for deaths vs income

```{r}
ggplot(death_rate_pills_income) +
  geom_point(aes(median_household_income, age_adjusted_rate)) +
  geom_smooth(aes(median_household_income, age_adjusted_rate), method = "lm", se = FALSE)  +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(labels = comma)  +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x="Average Annual Pills Per Person", y="age_adjusted Annual Opioid Death Rate", title="", caption = "Source: DEA ARCOS database, via Washington Post")
```

# Task: make a scatterplot for shipments v income

```{r}
ggplot(death_rate_pills_income) +
  geom_point(aes(average_pills_per_person, age_adjusted_rate)) +
  geom_smooth(aes(average_pills_per_person, age_adjusted_rate), method = "lm", se = FALSE)  +
  scale_y_continuous(labels = comma) +
  scale_x_continuous(labels = comma)  +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x="Average Annual Pills Per Person", y="age_adjusted Annual Opioid Death Rate", title="", caption = "Source: DEA ARCOS database, via Washington Post")
```

# Build a table that has all three. 

```{r}
death_rate_pills_income %>%
  ungroup() %>%
  select(median_household_income, age_adjusted_rate, average_pills_per_person) %>%
  correlate()
```

# Task: Go out and get one more census variable. What is it

# For Loops

Repeatable 

```{r}

statelist <- c("Maryland", "Virginia","Texas")

for (state in statelist) {
  print(state)
}

```

```{r}
statelist <- c("Maryland", "Virginia","Texas")

for (name in statelist) {
  print(name)
}

```

```{r}

arcos_state_pills_per_year <- arcos_county_pills_per_year %>%
  group_by(buyer_state, year) %>%
  summarise(total_pills = sum(dosage_unit))

```

``` {r}

arcos_state_pills_per_year %>%
  filter(buyer_state == "MD") %>%
  ggplot() + 
  geom_bar(stat="identity", aes(year, total_pills), fill="royal blue") +
  labs(x="Year", y="Total pills", title="") +
  scale_x_continuous(breaks = c(2006, 2007, 2008, 2009, 2010, 2011, 2012)) +
  scale_y_continuous(labels = comma)

```
```{r}

arcos_state_pills_per_year %>%
  filter(buyer_state == "MD" | buyer_state == "TX" | buyer_state == "VA") %>%
  ggplot() + 
  geom_bar(stat="identity", aes(year, total_pills), fill="royal blue") +
  facet_grid(. ~ buyer_state) +
  labs(x="Year", y="Total pills", title="") +
  scale_x_continuous(breaks = c(2006, 2007, 2008, 2009, 2010, 2011, 2012)) +
  scale_y_continuous(labels = comma)

```

```{r}

statelist <- c("MD", "VA","TX")

for (state in statelist) {

plot <- arcos_state_pills_per_year %>%
  filter(buyer_state == state) %>%
  ggplot() + 
  geom_bar(stat="identity", aes(year, total_pills), fill="royal blue") +
  facet_grid(. ~ buyer_state) +
  labs(x="Year", y="Total pills", title="") +
  scale_x_continuous(breaks = c(2006, 2007, 2008, 2009, 2010, 2011, 2012)) +
  scale_y_continuous(labels = comma)

print(plot)

}



```

# Task: what other cases could using a forloop be good for.  


## Submission

Save the R Markdown file.  Knit it to HTML and make sure it compiles correctly. Upload to GitHub, as instructed.  Provide links to GitHub in ELMS.   
